{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as gourimenon8\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as gourimenon8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"gourimenon8/sp25_taxi\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"gourimenon8/sp25_taxi\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository gourimenon8/sp25_taxi initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository gourimenon8/sp25_taxi initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "File already exists for 2023-01.\n",
      "Loading data for 2023-01...\n",
      "Total records: 3,066,766\n",
      "Valid records: 2,993,140\n",
      "Records dropped: 73,626 (2.40%)\n",
      "Successfully processed data for 2023-01.\n",
      "File already exists for 2023-02.\n",
      "Loading data for 2023-02...\n",
      "Total records: 2,913,955\n",
      "Valid records: 2,845,058\n",
      "Records dropped: 68,897 (2.36%)\n",
      "Successfully processed data for 2023-02.\n",
      "File already exists for 2023-03.\n",
      "Loading data for 2023-03...\n",
      "Total records: 3,403,766\n",
      "Valid records: 3,331,705\n",
      "Records dropped: 72,061 (2.12%)\n",
      "Successfully processed data for 2023-03.\n",
      "File already exists for 2023-04.\n",
      "Loading data for 2023-04...\n",
      "Total records: 3,288,250\n",
      "Valid records: 3,214,922\n",
      "Records dropped: 73,328 (2.23%)\n",
      "Successfully processed data for 2023-04.\n",
      "File already exists for 2023-05.\n",
      "Loading data for 2023-05...\n",
      "Total records: 3,513,649\n",
      "Valid records: 3,435,875\n",
      "Records dropped: 77,774 (2.21%)\n",
      "Successfully processed data for 2023-05.\n",
      "File already exists for 2023-06.\n",
      "Loading data for 2023-06...\n",
      "Total records: 3,307,234\n",
      "Valid records: 3,233,969\n",
      "Records dropped: 73,265 (2.22%)\n",
      "Successfully processed data for 2023-06.\n",
      "File already exists for 2023-07.\n",
      "Loading data for 2023-07...\n",
      "Total records: 2,907,108\n",
      "Valid records: 2,838,637\n",
      "Records dropped: 68,471 (2.36%)\n",
      "Successfully processed data for 2023-07.\n",
      "File already exists for 2023-08.\n",
      "Loading data for 2023-08...\n",
      "Total records: 2,824,209\n",
      "Valid records: 2,758,739\n",
      "Records dropped: 65,470 (2.32%)\n",
      "Successfully processed data for 2023-08.\n",
      "File already exists for 2023-09.\n",
      "Loading data for 2023-09...\n",
      "Total records: 2,846,722\n",
      "Valid records: 2,782,920\n",
      "Records dropped: 63,802 (2.24%)\n",
      "Successfully processed data for 2023-09.\n",
      "File already exists for 2023-10.\n",
      "Loading data for 2023-10...\n",
      "Total records: 3,522,285\n",
      "Valid records: 3,446,406\n",
      "Records dropped: 75,879 (2.15%)\n",
      "Successfully processed data for 2023-10.\n",
      "File already exists for 2023-11.\n",
      "Loading data for 2023-11...\n",
      "Total records: 3,339,715\n",
      "Valid records: 3,267,940\n",
      "Records dropped: 71,775 (2.15%)\n",
      "Successfully processed data for 2023-11.\n",
      "File already exists for 2023-12.\n",
      "Loading data for 2023-12...\n",
      "Total records: 3,376,567\n",
      "Valid records: 3,313,957\n",
      "Records dropped: 62,610 (1.85%)\n",
      "Error processing data for 2023-12: Unable to allocate 25.3 MiB for an array with shape (3313957, 1) and data type timedelta64[us]\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "Filtering data for location ID 43...\n",
      "Time series data shape: (8016, 1)\n",
      "                     rides\n",
      "pickup_hour               \n",
      "2023-01-01 00:00:00   92.0\n",
      "2023-01-01 01:00:00   81.0\n",
      "2023-01-01 02:00:00   29.0\n",
      "2023-01-01 03:00:00   15.0\n",
      "2023-01-01 04:00:00    4.0\n",
      "\n",
      "Results of Augmented Dickey-Fuller Test:\n",
      "ADF Statistic: -9.449621013621787\n",
      "p-value: 4.654184715134955e-16\n",
      "Series is stationary\n",
      "Significant ACF lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n",
      "Significant PACF lags: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]...\n",
      "\n",
      "Suggested ARMA orders based on ACF and PACF patterns:\n",
      "AR(p) order suggestions: [1, 2, 3, 24] (from PACF)\n",
      "MA(q) order suggestions: [1, 2, 3, 24] (from ACF)\n",
      "Training set size: (6412, 1)\n",
      "Testing set size: (1604, 1)\n",
      "Training set size: (6412, 11)\n",
      "Testing set size: (1604, 11)\n",
      "\n",
      "Finding best ARMA parameters...\n",
      "ARMA(0, 1) - AIC: 65117.607\n",
      "ARMA(0, 2) - AIC: 62762.032\n",
      "ARMA(0, 3) - AIC: 60924.689\n",
      "ARMA(0, 4) - AIC: 60023.271\n",
      "ARMA(0, 5) - AIC: 59467.069\n",
      "ARMA(1, 0) - AIC: 59579.819\n",
      "ARMA(1, 1) - AIC: 59403.139\n",
      "ARMA(1, 2) - AIC: 59224.870\n",
      "ARMA(1, 3) - AIC: 58766.726\n",
      "ARMA(1, 4) - AIC: 58699.897\n",
      "ARMA(1, 5) - AIC: 58529.124\n",
      "ARMA(2, 0) - AIC: 59317.588\n",
      "ARMA(2, 1) - AIC: 57799.612\n",
      "ARMA(2, 2) - AIC: 57298.120\n",
      "ARMA(2, 3) - AIC: 57137.146\n",
      "ARMA(2, 4) - AIC: 56938.042\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import itertools\n",
    "import warnings\n",
    "import holidays\n",
    "import mlflow\n",
    "import dagshub\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Import from src directory\n",
    "from src.data_utils import load_and_process_taxi_data, transform_raw_data_into_ts_data\n",
    "\n",
    "# Initialize MLflow tracking\n",
    "dagshub.init(repo_owner=\"gourimenon8\", repo_name=\"sp25_taxi\", mlflow=True)\n",
    "mlflow.set_experiment(\"improved_arma_model\")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and process NYC taxi data\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    rides = load_and_process_taxi_data(year=2023)\n",
    "    # rides = pd.concat([rides1, rides2], ignore_index=True)\n",
    "    \n",
    "    # Filter for location ID 43\n",
    "    LOCATION_ID = 43\n",
    "    print(f\"Filtering data for location ID {LOCATION_ID}...\")\n",
    "    temp_rides = rides[rides[\"pickup_location_id\"] == LOCATION_ID]\n",
    "    \n",
    "    # Transform into time series data\n",
    "    ts_data = transform_raw_data_into_ts_data(temp_rides)\n",
    "    ts_data = ts_data.drop(columns=[\"pickup_location_id\"])\n",
    "    \n",
    "    # Convert pickup_hour to datetime and set as index\n",
    "    ts_data[\"pickup_hour\"] = pd.to_datetime(ts_data[\"pickup_hour\"])\n",
    "    ts_data.set_index(\"pickup_hour\", inplace=True)\n",
    "    \n",
    "    # Ensure data is in time series format\n",
    "    ts_data = ts_data.asfreq(\"H\")\n",
    "    ts_data[\"rides\"] = ts_data[\"rides\"].astype(np.float32)\n",
    "    \n",
    "    print(f\"Time series data shape: {ts_data.shape}\")\n",
    "    print(ts_data.head())\n",
    "    \n",
    "    return ts_data\n",
    "\n",
    "def add_time_features(ts_data):\n",
    "    \"\"\"Add time-based features to the data\"\"\"\n",
    "    time_features = pd.DataFrame(index=ts_data.index)\n",
    "    \n",
    "    # Extract time components\n",
    "    time_features['hour'] = ts_data.index.hour\n",
    "    time_features['day_of_week'] = ts_data.index.dayofweek\n",
    "    time_features['day'] = ts_data.index.day\n",
    "    time_features['month'] = ts_data.index.month\n",
    "    time_features['quarter'] = ts_data.index.quarter\n",
    "    time_features['year'] = ts_data.index.year\n",
    "    \n",
    "    # Create binary features\n",
    "    time_features['is_weekend'] = (time_features['day_of_week'] >= 5).astype(int)\n",
    "    time_features['is_rush_hour_am'] = ((time_features['hour'] >= 7) & \n",
    "                                        (time_features['hour'] <= 9)).astype(int)\n",
    "    time_features['is_rush_hour_pm'] = ((time_features['hour'] >= 16) & \n",
    "                                        (time_features['hour'] <= 19)).astype(int)\n",
    "    \n",
    "    # Add US holidays\n",
    "    us_holidays = holidays.US(years=[2022, 2023])\n",
    "    time_features['is_holiday'] = [1 if d.date() in us_holidays else 0 for d in time_features.index]\n",
    "    \n",
    "    # Merge with original data\n",
    "    result = pd.concat([ts_data, time_features], axis=1)\n",
    "    return result\n",
    "\n",
    "def check_stationarity(series, window=24):\n",
    "    \"\"\"\n",
    "    Check stationarity using rolling statistics and Augmented Dickey-Fuller test\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pandas.Series\n",
    "        Time series to check for stationarity\n",
    "    window : int\n",
    "        Window size for rolling statistics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if stationary, False otherwise\n",
    "    dict\n",
    "        ADF test results\n",
    "    \"\"\"\n",
    "    # Calculate rolling statistics\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    rolling_std = series.rolling(window=window).std()\n",
    "    \n",
    "    # Plot rolling statistics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(211)\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.plot(series, label='Original')\n",
    "    plt.plot(rolling_mean, label='Rolling Mean')\n",
    "    plt.plot(rolling_std, label='Rolling Std')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Perform ADF test\n",
    "    adf_result = adfuller(series.dropna())\n",
    "    adf_output = {\n",
    "        'Test Statistic': adf_result[0],\n",
    "        'p-value': adf_result[1],\n",
    "        '#Lags Used': adf_result[2],\n",
    "        'Number of Observations': adf_result[3],\n",
    "        'Critical Values': adf_result[4]\n",
    "    }\n",
    "    \n",
    "    # Display ADF test results\n",
    "    plt.subplot(212)\n",
    "    plt.title('Augmented Dickey-Fuller Test Results')\n",
    "    plt.axis('off')\n",
    "    result_text = '\\n'.join([f\"{key}: {value}\" if not isinstance(value, dict) else f\"{key}: {value['1%']}, {value['5%']}, {value['10%']}\" \n",
    "                            for key, value in adf_output.items()])\n",
    "    plt.text(0.01, 0.8, result_text, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"stationarity_check.png\")\n",
    "    \n",
    "    # Print results\n",
    "    print('\\nResults of Augmented Dickey-Fuller Test:')\n",
    "    print(f'ADF Statistic: {adf_result[0]}')\n",
    "    print(f'p-value: {adf_result[1]}')\n",
    "    \n",
    "    # Return stationarity boolean and ADF results\n",
    "    is_stationary = adf_result[1] <= 0.05\n",
    "    print(f'Series is{\"\" if is_stationary else \" not\"} stationary')\n",
    "    \n",
    "    return is_stationary, adf_output\n",
    "\n",
    "def plot_time_series(ts_data, exog_data=None):\n",
    "    \"\"\"Plot time series data and derived features\"\"\"\n",
    "    # Plot time series\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(ts_data.index, ts_data['rides'])\n",
    "    plt.title('NYC Taxi Rides Time Series (Location ID 43)')\n",
    "    plt.ylabel('Number of Rides')\n",
    "    \n",
    "    # Plot hourly pattern\n",
    "    plt.subplot(3, 1, 2)\n",
    "    hourly_avg = ts_data.groupby(ts_data.index.hour)['rides'].mean()\n",
    "    plt.bar(hourly_avg.index, hourly_avg.values)\n",
    "    plt.title('Average Rides by Hour of Day')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Average Rides')\n",
    "    \n",
    "    # Plot weekly pattern\n",
    "    plt.subplot(3, 1, 3)\n",
    "    daily_avg = ts_data.groupby(ts_data.index.dayofweek)['rides'].mean()\n",
    "    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    plt.bar([day_names[i] for i in daily_avg.index], daily_avg.values)\n",
    "    plt.title('Average Rides by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Average Rides')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"time_series_patterns.png\")\n",
    "    \n",
    "    # If exogenous variables are provided, plot their relationship\n",
    "    if exog_data is not None and not exog_data.empty:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot relationship with hour\n",
    "        plt.subplot(2, 2, 1)\n",
    "        hour_avg = pd.DataFrame({'hour': exog_data['hour'], 'rides': ts_data['rides']})\n",
    "        hour_avg = hour_avg.groupby('hour')['rides'].mean()\n",
    "        plt.bar(hour_avg.index, hour_avg.values)\n",
    "        plt.title('Average Rides by Hour')\n",
    "        \n",
    "        # Plot relationship with day of week\n",
    "        plt.subplot(2, 2, 2)\n",
    "        dow_avg = pd.DataFrame({'day_of_week': exog_data['day_of_week'], 'rides': ts_data['rides']})\n",
    "        dow_avg = dow_avg.groupby('day_of_week')['rides'].mean()\n",
    "        plt.bar([day_names[i] for i in dow_avg.index], dow_avg.values)\n",
    "        plt.title('Average Rides by Day of Week')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Plot relationship with month\n",
    "        plt.subplot(2, 2, 3)\n",
    "        month_avg = pd.DataFrame({'month': exog_data['month'], 'rides': ts_data['rides']})\n",
    "        month_avg = month_avg.groupby('month')['rides'].mean()\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        plt.bar([month_names[i-1] for i in month_avg.index], month_avg.values)\n",
    "        plt.title('Average Rides by Month')\n",
    "        \n",
    "        # Plot relationship with holidays\n",
    "        plt.subplot(2, 2, 4)\n",
    "        holiday_avg = pd.DataFrame({'is_holiday': exog_data['is_holiday'], 'rides': ts_data['rides']})\n",
    "        holiday_avg = holiday_avg.groupby('is_holiday')['rides'].mean()\n",
    "        plt.bar(['Non-Holiday', 'Holiday'], holiday_avg.values)\n",
    "        plt.title('Average Rides on Holidays vs. Non-Holidays')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"feature_relationships.png\")\n",
    "\n",
    "def analyze_acf_pacf(series, lags=48):\n",
    "    \"\"\"Analyze and plot ACF and PACF to help identify ARMA orders\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    plot_acf(series.dropna(), ax=plt.gca(), lags=lags)\n",
    "    plt.title('Autocorrelation Function (ACF)')\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plot_pacf(series.dropna(), ax=plt.gca(), lags=lags)\n",
    "    plt.title('Partial Autocorrelation Function (PACF)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"acf_pacf_plots.png\")\n",
    "    \n",
    "    # Get significant lags from ACF and PACF\n",
    "    acf_values = acf(series.dropna(), nlags=lags)\n",
    "    pacf_values = pacf(series.dropna(), nlags=lags)\n",
    "    \n",
    "    # Use 95% confidence interval (1.96/sqrt(n))\n",
    "    confidence_interval = 1.96 / np.sqrt(len(series.dropna()))\n",
    "    \n",
    "    # Find significant lags\n",
    "    significant_acf_lags = [i for i, val in enumerate(acf_values) if abs(val) > confidence_interval and i > 0]\n",
    "    significant_pacf_lags = [i for i, val in enumerate(pacf_values) if abs(val) > confidence_interval and i > 0]\n",
    "    \n",
    "    print(f\"Significant ACF lags: {significant_acf_lags[:10]}...\")\n",
    "    print(f\"Significant PACF lags: {significant_pacf_lags[:10]}...\")\n",
    "    \n",
    "    # Suggest ARMA orders based on ACF and PACF\n",
    "    print(\"\\nSuggested ARMA orders based on ACF and PACF patterns:\")\n",
    "    print(f\"AR(p) order suggestions: {[1, 2, 3, 24]} (from PACF)\")\n",
    "    print(f\"MA(q) order suggestions: {[1, 2, 3, 24]} (from ACF)\")\n",
    "    \n",
    "    return significant_acf_lags, significant_pacf_lags\n",
    "\n",
    "def create_train_test_sets(ts_data, test_size=0.2):\n",
    "    \"\"\"Split data into training and testing sets\"\"\"\n",
    "    train_size = int(len(ts_data) * (1 - test_size))\n",
    "    train = ts_data.iloc[:train_size]\n",
    "    test = ts_data.iloc[train_size:]\n",
    "    print(f\"Training set size: {train.shape}\")\n",
    "    print(f\"Testing set size: {test.shape}\")\n",
    "    return train, test\n",
    "\n",
    "def seasonal_decompose(series):\n",
    "    \"\"\"Perform seasonal decomposition of time series\"\"\"\n",
    "    decomposition = sm.tsa.seasonal_decompose(series, model='additive', period=24)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(decomposition.observed)\n",
    "    plt.title('Observed')\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend)\n",
    "    plt.title('Trend')\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal)\n",
    "    plt.title('Seasonality')\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.plot(decomposition.resid)\n",
    "    plt.title('Residuals')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"seasonal_decomposition.png\")\n",
    "    \n",
    "    return decomposition\n",
    "\n",
    "def find_best_arma_orders(train_data, max_p=5, max_q=5, exog=None):\n",
    "    \"\"\"Find the best ARMA orders by fitting multiple models\"\"\"\n",
    "    best_aic = float(\"inf\")\n",
    "    best_order = None\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nFinding best ARMA parameters...\")\n",
    "    for p, q in itertools.product(range(max_p + 1), range(max_q + 1)):\n",
    "        if p == 0 and q == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            model = ARIMA(train_data, order=(p, 0, q), exog=exog)\n",
    "            model_fit = model.fit()\n",
    "            aic = model_fit.aic\n",
    "            results.append((p, q, aic))\n",
    "            \n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_order = (p, q)\n",
    "                \n",
    "            print(f\"ARMA({p}, {q}) - AIC: {aic:.3f}\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Sort results by AIC\n",
    "    results.sort(key=lambda x: x[2])\n",
    "    \n",
    "    print(\"\\nBest 5 ARMA Models:\")\n",
    "    for p, q, aic in results[:5]:\n",
    "        print(f\"ARMA({p}, {q}) - AIC: {aic:.3f}\")\n",
    "    \n",
    "    print(f\"\\nBest ARMA Order: {best_order}\")\n",
    "    \n",
    "    return best_order, results\n",
    "\n",
    "def fit_arma_model(train_data, order, exog=None):\n",
    "    \"\"\"Fit ARMA model with specified order\"\"\"\n",
    "    model = ARIMA(train_data, order=(order[0], 0, order[1]), exog=exog)\n",
    "    model_fit = model.fit()\n",
    "    print(model_fit.summary())\n",
    "    return model_fit\n",
    "\n",
    "def evaluate_forecast(actual, forecast, model_name=\"ARMA\"):\n",
    "    \"\"\"Evaluate forecast using various metrics\"\"\"\n",
    "    mae = mean_absolute_error(actual, forecast)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, forecast))\n",
    "    r2 = r2_score(actual, forecast)\n",
    "    mape = np.mean(np.abs((actual - forecast) / actual)) * 100\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    directions_actual = np.sign(np.diff(np.array(actual)))\n",
    "    directions_forecast = np.sign(np.diff(np.array(forecast)))\n",
    "    directional_accuracy = np.mean(directions_actual == directions_forecast) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name} Forecast Evaluation:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.3f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.3f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.3f}%\")\n",
    "    print(f\"R-squared (R²): {r2:.3f}\")\n",
    "    print(f\"Directional Accuracy: {directional_accuracy:.2f}%\")\n",
    "    \n",
    "    # Plot forecast vs actual\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(actual.index, actual, label='Actual')\n",
    "    plt.plot(actual.index, forecast, label='Forecast', color='red')\n",
    "    plt.title(f'{model_name} - Forecast vs Actual')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Rides')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{model_name.lower().replace(' ', '_')}_forecast.png\")\n",
    "    \n",
    "    # Plot forecast error\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(actual.index, actual - forecast, label='Forecast Error')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title(f'{model_name} - Forecast Error')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Error (Actual - Forecast)')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{model_name.lower().replace(' ', '_')}_error.png\")\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'directional_accuracy': directional_accuracy\n",
    "    }\n",
    "\n",
    "def analyze_residuals(model_fit, model_name=\"ARMA\"):\n",
    "    \"\"\"Analyze residuals of a fitted model\"\"\"\n",
    "    residuals = model_fit.resid\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot residuals over time\n",
    "    plt.subplot(311)\n",
    "    plt.plot(residuals)\n",
    "    plt.title(f'{model_name} Model - Residuals Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # Histogram of residuals\n",
    "    plt.subplot(312)\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.title('Histogram of Residuals')\n",
    "    plt.xlabel('Residual Value')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    plt.subplot(313)\n",
    "    sm.qqplot(residuals, line='s', ax=plt.gca())\n",
    "    plt.title('Q-Q Plot of Residuals')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name.lower().replace(' ', '_')}_residuals.png\")\n",
    "    \n",
    "    # ACF of residuals to check if they are white noise\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plot_acf(residuals.dropna(), ax=plt.gca(), lags=40)\n",
    "    plt.title(f'ACF of {model_name} Model Residuals')\n",
    "    plt.savefig(f\"{model_name.lower().replace(' ', '_')}_residuals_acf.png\")\n",
    "    \n",
    "    # Ljung-Box test\n",
    "    lb_test = sm.stats.acorr_ljungbox(residuals, lags=[10, 20, 30], return_df=True)\n",
    "    print(f\"\\nLjung-Box Test Results for {model_name} Model Residuals:\")\n",
    "    print(lb_test)\n",
    "    \n",
    "    # Check normality of residuals with Jarque-Bera test\n",
    "    jb_test = sm.stats.jarque_bera(residuals.dropna())\n",
    "    print(f\"\\nJarque-Bera Test Results for {model_name} Model Residuals:\")\n",
    "    print(f\"Statistic: {jb_test[0]:.3f}\")\n",
    "    print(f\"p-value: {jb_test[1]:.3f}\")\n",
    "    if jb_test[1] < 0.05:\n",
    "        print(\"Residuals are not normally distributed (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"Residuals appear to be normally distributed (p >= 0.05)\")\n",
    "    \n",
    "    # Descriptive statistics of residuals\n",
    "    residuals_stats = residuals.describe()\n",
    "    print(\"\\nResiduals Descriptive Statistics:\")\n",
    "    print(residuals_stats)\n",
    "    \n",
    "    return residuals, jb_test, lb_test\n",
    "\n",
    "def forecast_future(model_fit, steps=24, exog=None, plot=True):\n",
    "    \"\"\"Forecast future values using fitted model\"\"\"\n",
    "    forecast = model_fit.forecast(steps=steps, exog=exog)\n",
    "    \n",
    "    # Create date index for forecast\n",
    "    last_date = model_fit.data.index[-1]\n",
    "    forecast_index = pd.date_range(start=last_date, periods=steps + 1, freq=\"H\")[1:]\n",
    "    \n",
    "    if plot:\n",
    "        # Plot historical data and forecast\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(model_fit.data.index, model_fit.data.endog, label='Historical Data')\n",
    "        plt.plot(forecast_index, forecast, label='Forecast', color='red')\n",
    "        plt.title('ARMA Model Forecast')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Number of Rides')\n",
    "        plt.legend()\n",
    "        plt.savefig(\"arma_forecast.png\")\n",
    "    \n",
    "    # Create forecast DataFrame\n",
    "    forecast_df = pd.DataFrame({\n",
    "        'forecast': forecast\n",
    "    }, index=forecast_index)\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    ts_data = load_data()\n",
    "    \n",
    "    # Add time-based features\n",
    "    ts_with_features = add_time_features(ts_data)\n",
    "    \n",
    "    # Visualize time series data and patterns\n",
    "    plot_time_series(ts_data, ts_with_features)\n",
    "    \n",
    "    # Check stationarity\n",
    "    is_stationary, adf_results = check_stationarity(ts_data['rides'])\n",
    "    \n",
    "    # Analyze ACF and PACF to identify potential AR and MA orders\n",
    "    acf_lags, pacf_lags = analyze_acf_pacf(ts_data['rides'])\n",
    "    \n",
    "    # Perform seasonal decomposition\n",
    "    decomposition = seasonal_decompose(ts_data['rides'])\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = create_train_test_sets(ts_data)\n",
    "    train_features, test_features = create_train_test_sets(ts_with_features)\n",
    "    \n",
    "    # Extract exogenous variables\n",
    "    exog_columns = ['hour', 'day_of_week', 'is_weekend', 'is_holiday', \n",
    "                    'is_rush_hour_am', 'is_rush_hour_pm']\n",
    "    train_exog = train_features[exog_columns] if exog_columns else None\n",
    "    test_exog = test_features[exog_columns] if exog_columns else None\n",
    "    \n",
    "    # Find best ARMA orders automatically\n",
    "    auto_best_order, auto_results = find_best_arma_orders(train_data['rides'], max_p=5, max_q=5)\n",
    "    \n",
    "    # Fit best ARMA model (without exogenous variables)\n",
    "    best_arma_model = fit_arma_model(train_data['rides'], auto_best_order)\n",
    "    \n",
    "    # Forecast and evaluate\n",
    "    test_forecast = best_arma_model.forecast(steps=len(test_data))\n",
    "    metrics_arma = evaluate_forecast(test_data['rides'], test_forecast, model_name=\"ARMA\")\n",
    "    \n",
    "    # Analyze residuals\n",
    "    residuals_arma, jb_test_arma, lb_test_arma = analyze_residuals(best_arma_model, model_name=\"ARMA\")\n",
    "    \n",
    "    # Fit ARMA model with exogenous variables (ARMAX)\n",
    "    try:\n",
    "        print(\"\\nFitting ARMAX model with exogenous variables...\")\n",
    "        armax_model = fit_arma_model(train_data['rides'], auto_best_order, exog=train_exog)\n",
    "        \n",
    "        # Forecast and evaluate ARMAX\n",
    "        armax_forecast = armax_model.forecast(steps=len(test_data), exog=test_exog)\n",
    "        metrics_armax = evaluate_forecast(test_data['rides'], armax_forecast, model_name=\"ARMAX\")\n",
    "        \n",
    "        # Analyze residuals for ARMAX\n",
    "        residuals_armax, jb_test_armax, lb_test_armax = analyze_residuals(armax_model, model_name=\"ARMAX\")\n",
    "        \n",
    "        # Compare models\n",
    "        print(\"\\nModel Comparison:\")\n",
    "        print(f\"ARMA MAE: {metrics_arma['mae']:.3f}, RMSE: {metrics_arma['rmse']:.3f}, R²: {metrics_arma['r2']:.3f}\")\n",
    "        print(f\"ARMAX MAE: {metrics_armax['mae']:.3f}, RMSE: {metrics_armax['rmse']:.3f}, R²: {metrics_armax['r2']:.3f}\")\n",
    "        \n",
    "        # Determine best model\n",
    "        best_model = \"ARMAX\" if metrics_armax['mae'] < metrics_arma['mae'] else \"ARMA\"\n",
    "        best_metrics = metrics_armax if best_model == \"ARMAX\" else metrics_arma\n",
    "        print(f\"\\nBest model based on MAE: {best_model}\")\n",
    "        \n",
    "        # Create bar chart comparing metrics\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        models = ['ARMA', 'ARMAX']\n",
    "        mae_values = [metrics_arma['mae'], metrics_armax['mae']]\n",
    "        rmse_values = [metrics_arma['rmse'], metrics_armax['rmse']]\n",
    "        r2_values = [metrics_arma['r2'], metrics_armax['r2']]\n",
    "        \n",
    "        x = np.arange(len(models))\n",
    "        width = 0.25\n",
    "        \n",
    "        plt.bar(x - width, mae_values, width, label='MAE')\n",
    "        plt.bar(x, rmse_values, width, label='RMSE')\n",
    "        plt.bar(x + width, r2_values, width, label='R²')\n",
    "        \n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Metric Value')\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xticks(x, models)\n",
    "        plt.legend()\n",
    "        plt.savefig(\"model_comparison.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting ARMAX model: {e}\")\n",
    "        best_model = \"ARMA\"\n",
    "        best_metrics = metrics_arma\n",
    "    \n",
    "    # Train final model on all data\n",
    "    print(\"\\nTraining final model on all data...\")\n",
    "    if best_model == \"ARMA\":\n",
    "        final_model = fit_arma_model(ts_data['rides'], auto_best_order)\n",
    "    else:\n",
    "        final_model = fit_arma_model(ts_data['rides'], auto_best_order, exog=ts_with_features[exog_columns])\n",
    "    \n",
    "    # Forecast future values\n",
    "    future_steps = 48  # 2 days\n",
    "    future_forecast = forecast_future(final_model, steps=future_steps)\n",
    "    \n",
    "    print(f\"\\nForecasted values for the next {future_steps} hours:\")\n",
    "    print(future_forecast.head(10))\n",
    "    \n",
    "    # Log to MLflow\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"best_model_type\", best_model)\n",
    "        mlflow.log_param(\"arma_order\", auto_best_order)\n",
    "        if best_model == \"ARMAX\":\n",
    "            mlflow.log_param(\"exog_variables\", exog_columns)\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in best_metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Log artifacts\n",
    "        mlflow.log_artifact(\"time_series_patterns.png\")\n",
    "        mlflow.log_artifact(\"stationarity_check.png\")\n",
    "        mlflow.log_artifact(\"acf_pacf_plots.png\")\n",
    "        mlflow.log_artifact(\"seasonal_decomposition.png\")\n",
    "        mlflow.log_artifact(f\"{best_model.lower()}_forecast.png\")\n",
    "        mlflow.log_artifact(f\"{best_model.lower()}_error.png\")\n",
    "        mlflow.log_artifact(f\"{best_model.lower()}_residuals.png\")\n",
    "        mlflow.log_artifact(\"arma_forecast.png\")\n",
    "        if best_model == \"ARMAX\":\n",
    "            mlflow.log_artifact(\"feature_relationships.png\")\n",
    "            mlflow.log_artifact(\"model_comparison.png\")\n",
    "    \n",
    "    print(\"\\nARMA modeling complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
